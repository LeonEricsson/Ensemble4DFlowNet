{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This file has been used to evaluate predictions but is currently not in a suitable state for other users, it is more of a personal\n",
    "playground than public evaluation tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import datetime\n",
    "import h5py\n",
    "from Network import loss_utils\n",
    "from utils import evaluation_utils as e_utils\n",
    "from skimage import morphology\n",
    "from scipy import stats\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 1200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = \"../data\"\n",
    "#data_dir = \"../THESIS_INVIVO_DATA/BRAIN/Healthy/HV01/h5\"\n",
    "data_dir = \"../results/Bagging-12\"\n",
    "\n",
    "hr_filename = \"P03_fft_SR.h5\"\n",
    "lr_filename = \"patient3-postOp_LR.h5\"\n",
    "prediction_filename = \"aorta03_SR.h5\"\n",
    "\n",
    "prediction_dirs = [\"../results/combined\", \"../results/bag-12\", \"../results/halfsubbag-12\"]\n",
    "\n",
    "ground_truth_file = f\"{data_dir}/{hr_filename}\"\n",
    "prediction_files = [f\"{prediction_dir}/{prediction_filename}\" for prediction_dir in prediction_dirs]\n",
    "lr_file = f\"{data_dir}/{lr_filename}\"\n",
    "\n",
    "# Parameters\n",
    "mask_threshold = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_h5(output_filepath, col_name, dataset, compression=None, delete=False):\n",
    "    # convert float64 to float32 to save space\n",
    "    if dataset.dtype == 'float64':\n",
    "        dataset = np.array(dataset, dtype='float32')\n",
    "    \n",
    "    with h5py.File(output_filepath, 'a') as hf:\n",
    "        if delete:\n",
    "            del hf[col_name]\n",
    "        hf.create_dataset(col_name, data=dataset, compression=compression) # gzip, compression_opts=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 4\n",
    "mask = None\n",
    "with h5py.File(\"../THESIS_INVIVO_DATA/THORAX/P01/h5/P01_aorta_HR.h5\", 'r') as hf:\n",
    "        print(hf.keys())\n",
    "        #print(np.array(hf['u_max']).shape)\n",
    "        u = np.array(hf['u'])\n",
    "        print(u.shape)\n",
    "#         v = np.array(hf['v_max'])\n",
    "#         w = np.array(hf['w_max'])\n",
    "#         # if 'mask' in hf.keys():\n",
    "#         #         mask = np.array(hf['mask'])\n",
    "#         del hf['u_max']\n",
    "#         del hf['v_max']\n",
    "#         del hf['w_max']\n",
    "\n",
    "\n",
    "# save_to_h5(f\"{data_dir}/HV01_HR.h5\", 'venc_u', u, compression='gzip')\n",
    "# save_to_h5(f\"{data_dir}/HV01_HR.h5\", 'venc_v', v, compression='gzip')\n",
    "# save_to_h5(f\"{data_dir}/HV01_HR.h5\", 'venc_w', w, compression='gzip')\n",
    "\n",
    "# if mask.any():\n",
    "#         save_to_h5(f\"{data_dir}/P01_cardiac_frame_8.h5\", 'mask', mask, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umax = []\n",
    "vmax = []\n",
    "wmax = []\n",
    "frames = 3\n",
    "for frame in range(frames):\n",
    "    with h5py.File(ground_truth_file, 'r') as hf:\n",
    "        u = np.array(hf['u'][frame])\n",
    "        v = np.array(hf['v'][frame])\n",
    "        w = np.array(hf['w'][frame])\n",
    "    umax.append(np.amax(abs(u)))\n",
    "    vmax.append(np.amax(abs(v)))\n",
    "    wmax.append(np.amax(abs(w)))\n",
    "umax = np.array(umax)\n",
    "vmax = np.array(vmax)\n",
    "wmax = np.array(wmax)\n",
    "print(umax)\n",
    "print(vmax)\n",
    "print(wmax)\n",
    "with h5py.File(ground_truth_file, 'a') as hf:\n",
    "    hf.create_dataset('u_max', data=umax, compression='gzip')\n",
    "    hf.create_dataset('v_max', data=vmax, compression='gzip')\n",
    "    hf.create_dataset('w_max', data=wmax, compression='gzip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = \"../THESIS_INVIVO_DATA/THORAX/P03/h5/P03_aorta_HR.h5\"\n",
    "with h5py.File(temp_file, 'r') as hf:\n",
    "    mask = tf.convert_to_tensor(hf['mask'])\n",
    "    mask = mask[tf.newaxis] if len(mask.shape) == 3 else mask\n",
    "    binary_mask = tf.cast((tf.cast(mask, dtype=tf.float32) >= mask_threshold), dtype=tf.float32)\n",
    "    data_count = len(hf.get(\"u\"))\n",
    "print(mask.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean speed (Vx, Vy, Vz, |V|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_speed = np.zeros((data_count, 4))\n",
    "\n",
    "peak_flow = -1\n",
    "peak_flow_idx = -1\n",
    "\n",
    "for idx in range(data_count):\n",
    "    \n",
    "    with h5py.File(ground_truth_file, mode = 'r' ) as hf:\n",
    "        hr_u = tf.convert_to_tensor(hf['u'][idx])[tf.newaxis]\n",
    "        hr_v = tf.convert_to_tensor(hf['v'][idx])[tf.newaxis]\n",
    "        hr_w = tf.convert_to_tensor(hf['w'][idx])[tf.newaxis]    \n",
    "\n",
    "    \n",
    "    binary_mask_frame = binary_mask if binary_mask.shape[0] == 1 else binary_mask[idx][tf.newaxis]\n",
    "    \n",
    "    # Average speed per frame across all axis\n",
    "    hr = tf.concat([hr_u, hr_v, hr_w], axis=0)\n",
    "    \n",
    "    squared = tf.map_fn(lambda x : tf.square(x), hr)\n",
    "    speed = tf.math.sqrt(tf.reduce_sum(squared, axis=0))\n",
    "    flow = tf.reduce_sum(speed, axis=[0,1,2]) / (tf.reduce_sum(binary_mask_frame, axis=[1,2,3]) + 1)*100\n",
    "\n",
    "    # Average speed per frame for each axis independetly.\n",
    "    flow_uvw = tf.reduce_sum(hr, axis=[1,2,3]) / (tf.reduce_sum(binary_mask_frame, axis=[1,2,3]) + 1)*100\n",
    "    \n",
    "    mean_speed[idx] = tf.concat([flow, flow_uvw], axis=0)\n",
    "    if peak_flow < flow:\n",
    "        peak_flow = flow\n",
    "        peak_flow_idx = idx\n",
    "print(\"Peak flow idx:\", peak_flow_idx)\n",
    "print(\"Plotting average speed...\")\n",
    "fig, ax = plt.subplots()\n",
    "filename_start = prediction_files[0].find(\"-\") + 1\n",
    "colors = ['r', 'g', 'b', 'y']\n",
    "labels = ['$\\mathregular{|V|}$', '$\\mathregular{V_x}$', '$\\mathregular{V_y}$', '$\\mathregular{V_z}$']\n",
    "for i in range(4):\n",
    "    ax.plot(tf.range(data_count), mean_speed[:, i], color=colors[i], label=labels[i])\n",
    "\n",
    "ax.set_xlabel(\"Frames\")\n",
    "ax.set_ylabel(\"Avg. speed (cm/s)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(f\"../evaluation/{prediction_files[0][filename_start:-3]}_speed.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_speed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative mean error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_err = np.zeros((len(prediction_files), data_count))\n",
    "mean_speed = np.zeros((data_count, 4))\n",
    "\n",
    "peak_flow = -1\n",
    "peak_flow_idx = -1\n",
    "\n",
    "for idx in range(data_count):\n",
    "    # Load the ground truth U V W from H5 and crop if necessary\n",
    "    for nr, prediction_file in enumerate(prediction_files):\n",
    "        # Load the prediction U V W from H5\n",
    "        with h5py.File(prediction_file, mode = 'r' ) as hf:\n",
    "            pred_u = tf.convert_to_tensor(hf['u'][idx])[tf.newaxis]\n",
    "            pred_v = tf.convert_to_tensor(hf['v'][idx])[tf.newaxis]\n",
    "            pred_w = tf.convert_to_tensor(hf['w'][idx])[tf.newaxis]\n",
    "\n",
    "        with h5py.File(ground_truth_file, mode = 'r' ) as hf:\n",
    "            hr_u = tf.convert_to_tensor(hf['u'][idx])[tf.newaxis]\n",
    "            hr_v = tf.convert_to_tensor(hf['v'][idx])[tf.newaxis]\n",
    "            hr_w = tf.convert_to_tensor(hf['w'][idx])[tf.newaxis]\n",
    "\n",
    "        # Relative error per frame\n",
    "        \n",
    "        rel_err[nr][idx] = (loss_utils.calculate_relative_error(pred_u, pred_v, pred_w, hr_u, hr_v, hr_w, binary_mask))\n",
    "    \n",
    "    \n",
    "    # Average speed per frame across all axis\n",
    "    hr = tf.concat([hr_u, hr_v, hr_w], axis=0)\n",
    "    squared = tf.map_fn(lambda x : tf.square(x), hr)\n",
    "    speed = tf.math.sqrt(tf.reduce_sum(squared, axis=0))\n",
    "    flow = tf.reduce_sum(speed, axis=[0,1,2]) / (tf.reduce_sum(binary_mask, axis=[1,2,3]) + 1)*100\n",
    "\n",
    "    # Average speed per frame for each axis independetly.\n",
    "    flow_uvw = tf.reduce_sum(hr, axis=[1,2,3]) / (tf.reduce_sum(binary_mask, axis=[1,2,3]) + 1)*100\n",
    "    \n",
    "    mean_speed[idx] = tf.concat([flow, flow_uvw], axis=0)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "print(f\"Plotting relative mean error...\")\n",
    "\n",
    "model_name_start = prediction_dirs[0].find(\"/\" ,prediction_dirs[0].find(\"/\")+1)+1\n",
    "\n",
    "for idx, row in enumerate(rel_err):\n",
    "    if data_count == 1:\n",
    "        ax.scatter(0, row, label=prediction_dirs[idx][model_name_start:])\n",
    "    else:\n",
    "        ax.plot(np.arange(data_count), row, label=prediction_dirs[idx][model_name_start:])\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.set_ybound(9, 35)\n",
    "ax.set_xticks([0], 0)\n",
    "ax.set_xlabel(\"Frame\")\n",
    "#plt.ylabel(\"Relative error (%)\")\n",
    "#plt.title(f\"Relative speed error ({prediction_filename[:-6]})\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig(f\"../evaluation/{prediction_filename[:-3]}_new_RME.png\", transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative slice comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "def get_slice_values(body, idx, axis='x'):\n",
    "    if axis=='x':\n",
    "        vals = body[idx, :, :]\n",
    "    elif axis=='y':\n",
    "        vals = body[:, idx, :]\n",
    "    elif axis=='z':\n",
    "        vals = body[:,:,idx]\n",
    "    else:\n",
    "        print(\"Error: x, y, z are available axes\")\n",
    "        return\n",
    "    return vals\n",
    "\n",
    "# Available vel_dirs are u, v, w.\n",
    "def slice(file, frame, idx, vel_dir='u', axis='x'):\n",
    "\n",
    "    with h5py.File(file, mode = 'r' ) as hf:\n",
    "        body = np.asarray(hf[vel_dir][frame])\n",
    "        sliced = get_slice_values(body, idx, axis)\n",
    "\n",
    "    return sliced\n",
    "\n",
    "# Available vel_dirs are u, v, w.\n",
    "def slice_mul(file, frame, rng, vel_dir='u', slice_axis='x'):\n",
    "\n",
    "    with h5py.File(file, mode = 'r' ) as hf:\n",
    "        data = np.asarray(hf[vel_dir][frame])\n",
    "    \n",
    "    if slice_axis=='y':\n",
    "        data = np.moveaxis(data, 1, 0)\n",
    "    elif slice_axis=='z':\n",
    "        data = np.moveaxis(data, 2, 0)\n",
    "    \n",
    "    return data[rng]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE and Relative Error in frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "data_dir = \"../data\"\n",
    "prediction_dir = \"../results/Bagging-2\"\n",
    "\n",
    "test_filenames = [\"aorta03\", \"patient3-postOp\", \"cardiacM4Dx2\"] # Assumed to be one file from each compartment\n",
    "test_frames = [0,34,14]\n",
    "\n",
    "# Parameters\n",
    "mask_threshold = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(hr, pred):\n",
    "    e = (pred - hr) ** 2 \n",
    "    \n",
    "    # Return error and its standard deviation\n",
    "    return e, np.std(e), np.var(e)\n",
    "\n",
    "# Frame of interest\n",
    "frame = 0\n",
    "\n",
    "# Track RMSE and Variance across testing files / compartments\n",
    "variance = {\"u\":0, \"v\": 0, \"w\": 0, \"uvw\": 0}\n",
    "rmse = {\"u\":0, \"v\": 0, \"w\": 0, \"uvw\": 0}\n",
    "total_rel = 0\n",
    "\n",
    "model = prediction_dir[prediction_dir.rindex(\"/\")+1:]\n",
    "print(f\"--- {model} prediction evaluation --- \")\n",
    "# Calculate for all prediction files\n",
    "for f, frame in zip(test_filenames, test_frames):\n",
    "    ground_truth_file = f\"{data_dir}/{f}_HR.h5\"\n",
    "    prediction_file = f\"{prediction_dir}/{f}_SR.h5\"\n",
    "\n",
    "    print(f\"File: {f}\")\n",
    "    # Load the prediction U V W from H5\n",
    "    with h5py.File(prediction_file, mode = 'r' ) as hf:\n",
    "        pred_u = tf.convert_to_tensor(hf['u'][frame])[tf.newaxis]\n",
    "        pred_v = tf.convert_to_tensor(hf['v'][frame])[tf.newaxis]\n",
    "        pred_w = tf.convert_to_tensor(hf['w'][frame])[tf.newaxis]\n",
    "\n",
    "    with h5py.File(ground_truth_file, mode = 'r' ) as hf:\n",
    "        hr_u = tf.convert_to_tensor(hf['u'][frame])[tf.newaxis]\n",
    "        hr_v = tf.convert_to_tensor(hf['v'][frame])[tf.newaxis]\n",
    "        hr_w = tf.convert_to_tensor(hf['w'][frame])[tf.newaxis]\n",
    "        \n",
    "        mask = tf.convert_to_tensor(hf['mask'])\n",
    "        mask = mask[tf.newaxis] if len(mask.shape) == 3 else mask\n",
    "        binary_mask = tf.cast((tf.cast(mask, dtype=tf.float32) >= mask_threshold), dtype=tf.float32)\n",
    "        data_count = len(hf.get(\"u\"))\n",
    "\n",
    "    # Mean Tangent Absolute Percentage Error\n",
    "    rel_err = (loss_utils.calculate_relative_error(pred_u, pred_v, pred_w, hr_u, hr_v, hr_w, binary_mask))\n",
    "    total_rel += rel_err.numpy()[0]\n",
    "    print(f\"Relative Error: {rel_err.numpy()[0]:.2f}%\")\n",
    "    \n",
    "    # Track error across velocity components for |V|\n",
    "    e_uvw = 0\n",
    "    for (component, pred, hr) in zip(['u', 'v', 'w'], [pred_u, pred_v, pred_w], [hr_u, hr_v, hr_w]):\n",
    "        e, std, var = calculate_error(pred, hr)\n",
    "        variance[component] += var\n",
    "        rmse[component] += np.sqrt(np.mean(e))\n",
    "        e_uvw += e\n",
    "        print(f\"{component} - RMSE + STD: {np.sqrt(np.mean(e)):.4f} + {std:.4f}\")\n",
    "    variance[\"uvw\"] += np.var(e_uvw)\n",
    "    rmse[\"uvw\"] += np.sqrt(np.mean(e_uvw))\n",
    "    print(f\"Total RMSE + STD: {np.sqrt(np.mean(e_uvw)):.4f} + {np.std(e_uvw):.4f}\")\n",
    "    print(\"\")\n",
    "  \n",
    "print(\"Average across compartments\")\n",
    "print(f\"Relative error: {total_rel/len(test_filenames):.2f}%\")\n",
    "for (c, var), (_, rmse) in zip(variance.items(), rmse.items()):\n",
    "    average_rmse = rmse / len(test_filenames)\n",
    "    average_var = var / len(test_filenames)\n",
    "    average_std = np.sqrt(average_var)\n",
    "    print(f\"{c} - RMSE + STD: {average_rmse:.4f} + {average_std:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "data_dir =\"../data\"\n",
    "eval_dirs = [\"../results/Aorta\", \"../results/Cerebro\", \"../results/Cardiac\", \"../results/Combined\", \"../results/Bagging-2\", \"../results/Stacking-2\"]#,\"../results/Stacking-3-archblock\"]# \"../results/Bagging-12\"]# \"../results/Combined\"]#,\"../results/Stacking-2\"]#, \"../results/stacking-4-archupsample\", \"../results/stacking-3-block-4-upsample\",]\n",
    "\n",
    "test_filenames = [(0, 68, \"aorta03\"), (34, 96, \"patient3-postOp\"), (16, 20, \"cardiacM4Dx2\")] # Assumed to be one file from each compartment\n",
    "invivo_filenames = [(1, 18, \"HV01\"), (5, 42, \"P02_aorta\"), (13, 26, \"P03_cardiac\")]\n",
    "invivo2_filenames = [(5, 12, \"HV03\"),(5, 32, \"P02_aorta\"), (8, 32, \"P02_cardiac\")]\n",
    "test_frames = [0,34,14]\n",
    "test_file_idx = 2\n",
    "\n",
    "invivo = False\n",
    "if not invivo:\n",
    "    frame, idx, test_file = test_filenames[test_file_idx] # in silico\n",
    "else:\n",
    "    data_dir = \"../THESIS_INVIVO_DATA/BRAIN/Healthy/HV01/h5\"\n",
    "    #data_dir = \"../THESIS_INVIVO_DATA/THORAX/P03/h5\"\n",
    "    frame, idx, test_file = invivo_filenames[test_file_idx] # in vivo\n",
    "\n",
    "    \n",
    "HR_file = f\"{data_dir}/{test_file}_HR.h5\"\n",
    "LR_file = f\"{data_dir}/{test_file}_LR.h5\"\n",
    "#LR_file = \"../THESIS_INVIVO_DATA/THORAX/P01/h5/P01_fft_LR.h5\"\n",
    "delim = test_file.find('_')\n",
    "test_file = test_file[:delim] if delim != -1 else test_file\n",
    "test_file += \"_HR_SR.h5\" if invivo else \"_SR.h5\"\n",
    "print(delim)\n",
    "print(test_file)\n",
    "HR_files = [f\"{data_dir}/{domain[2]}_HR.h5\" for domain in test_filenames]\n",
    "test_files = [f\"{domain[2]}_SR.h5\" for domain in test_filenames]\n",
    "eval_files = [f\"{dir}/{test_file}\" for dir in eval_dirs]\n",
    "print(eval_files)\n",
    "eval_files_for_all_HR = [[f\"{dir}/{file}\" for dir in eval_dirs] for file in test_files]\n",
    "\n",
    "\n",
    "# Parameters\n",
    "mask_threshold = 0.25\n",
    "\n",
    "#frame = 0\n",
    "#idx = 50\n",
    "vel_dir = 'u'\n",
    "slice_axis = 'x'\n",
    "# Load mask\n",
    "with h5py.File(HR_file, 'r') as hf:\n",
    "    mask = tf.convert_to_tensor(hf['mask'])\n",
    "    mask = mask[tf.newaxis] if len(mask.shape) == 3 else mask\n",
    "    binary_mask = tf.cast((tf.cast(mask, dtype=tf.float32) >= mask_threshold), dtype=tf.float32)\n",
    "    data_count = len(hf.get(\"u\"))\n",
    "print(mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# frame = 3\n",
    "# idx = 20\n",
    "#HR_file = \"../THESIS_INVIVO_DATA/THORAX/P01/h5/P01_cardiac_HR.h5\"\n",
    "\n",
    "delim = \"results/\"\n",
    "stop_delim = len(test_file) +1\n",
    "start = eval_files[0].find(delim) + len(delim)\n",
    "HR_slice = slice(HR_file, frame, idx, vel_dir, slice_axis)\n",
    "HR_slice = HR_slice[50:132, 60:120]\n",
    "print(HR_slice)\n",
    "#LR_slice = slice(LR_file, frame, int(idx/2), vel_dir, slice_axis)\n",
    "_min, _max = 99, -1\n",
    "_min, _max = min(_min, np.amin(HR_slice)), max(_max, np.amax(HR_slice))\n",
    "print(_min, _max)\n",
    "#_min, _max = min(_min, np.amin(LR_slice)), max(_max, np.amax(LR_slice))\n",
    "#print(_min, _max)\n",
    "skip = 0\n",
    "for file in eval_files:\n",
    "    if skip:\n",
    "        skip -= 1\n",
    "        continue\n",
    "    sliced = slice(file, frame, idx*2, vel_dir, slice_axis)\n",
    "    _min, _max = min(_min, np.amin(sliced)), max(_max, np.amax(sliced))\n",
    "    print(np.amin(sliced),np.amax(sliced))\n",
    "#skip = 2\n",
    "fig, ax = plt.subplots()\n",
    "fig.tight_layout()\n",
    "ax.set_axis_off()\n",
    "print(\"Final:\",_min, _max)\n",
    "_min = -0.6\n",
    "_max = 0.7\n",
    "t = ax.imshow(HR_slice, interpolation='nearest', cmap='viridis', origin='lower', vmin=_min, vmax=_max)\n",
    "#fig.colorbar(t, ax=ax, orientation=\"horizontal\")\n",
    "#fig.savefig(f\"../evaluation/invivo/SRBeyond/HV01_1_18/NR_zoom1\")\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.tight_layout()\n",
    "# ax.set_axis_off()\n",
    "#test = ax.imshow(LR_slice, interpolation='nearest', cmap='viridis', origin='lower', vmin=_min, vmax=_max)\n",
    "#fig.colorbar(test, ax=ax)\n",
    "#fig.savefig(f\"../evaluation/invivo/Recovery/P03_cardiac_13_26/LR\")\n",
    "index = 1\n",
    "for file in eval_files:\n",
    "    if skip:\n",
    "        skip -= 1\n",
    "        continue\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.tight_layout()\n",
    "    ax.set_axis_off()\n",
    "    sliced = slice(file, frame, 2*idx, vel_dir, slice_axis)\n",
    "    sliced = sliced[100:264,120:240]\n",
    "    ax.imshow(sliced, interpolation='nearest', cmap='viridis', origin='lower', vmin=_min, vmax=_max)\n",
    "    \n",
    "    #fig.savefig(f\"../evaluation/invivo/SRBeyond/HV01_1_18/{index}_zoom1\")\n",
    "    index += 1\n",
    "\n",
    "# axis.imshow(LR_slice, interpolation='nearest', cmap='viridis', origin='lower', vmin=0, vmax=1)\n",
    "\n",
    "# for ax, file in zip(axes.flatten(), eval_files):\n",
    "#     sliced = slice(file, frame, idx, vel_dir, slice_axis)\n",
    "#     diff_sliced = abs(HR_slice - sliced)\n",
    "#     _min, _max = min(_min, np.amin(diff_sliced)), max(_max, np.amax(diff_sliced))\n",
    "# test = []\n",
    "# # for ax, file in zip(axes.flatten(), eval_files):\n",
    "#     #ax.set_axis_off()\n",
    "# file = eval_files[0]\n",
    "# sliced = slice(file, frame, idx, vel_dir, slice_axis)\n",
    "# diff_sliced = abs(HR_slice - sliced)\n",
    "# test = axis.imshow(diff_sliced, interpolation='nearest', cmap='viridis', origin='lower', vmin=_min, vmax=_max)\n",
    "\n",
    "#fig.colorbar(test, ax=axes[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delim = \"results/\"\n",
    "stop_delim = len(test_file) +1\n",
    "start = eval_files[0].find(delim) + len(delim)\n",
    "HR_slice = slice(HR_file, frame, idx, vel_dir, slice_axis)\n",
    "#LR_slice = slice(LR_file, frame, int(idx/2), vel_dir, slice_axis)\n",
    "_min, _max = 99, -1\n",
    "_min, _max = min(_min, np.amin(HR_slice)), max(_max, np.amax(HR_slice))\n",
    "print(_min, _max)\n",
    "#_min, _max = min(_min, np.amin(LR_slice)), max(_max, np.amax(LR_slice))\n",
    "#print(_min, _max)\n",
    "skip = 0\n",
    "for file in eval_files:\n",
    "    if skip:\n",
    "        skip -= 1\n",
    "        continue\n",
    "    sliced = slice(file, frame, idx, vel_dir, slice_axis)\n",
    "    sliced = abs(HR_slice-sliced)\n",
    "    _min, _max = min(_min, np.amin(sliced)), max(_max, np.amax(sliced))\n",
    "    print(np.amin(sliced),np.amax(sliced))\n",
    "#skip = 2\n",
    "fig, ax = plt.subplots()\n",
    "fig.tight_layout()\n",
    "ax.set_axis_off()\n",
    "print(\"Final:\",_min, _max)\n",
    "_min = 0\n",
    "_max = 1\n",
    "t = ax.imshow(HR_slice, interpolation='nearest', cmap='viridis', origin='lower', vmin=_min, vmax=_max)\n",
    "fig.colorbar(t, ax=ax, orientation=\"horizontal\")\n",
    "fig.savefig(f\"../evaluation/insilico/baseVScombVSens/abs/Aorta03/colorbar\")\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.tight_layout()\n",
    "# ax.set_axis_off()\n",
    "#test = ax.imshow(LR_slice, interpolation='nearest', cmap='viridis', origin='lower', vmin=_min, vmax=_max)\n",
    "#fig.colorbar(test, ax=ax)\n",
    "#fig.savefig(f\"../evaluation/invivo/Recovery/P03_cardiac_13_26/LR\")\n",
    "index = 1\n",
    "for file in eval_files:\n",
    "    print(file)\n",
    "    if skip:\n",
    "        skip -= 1\n",
    "        continue\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.tight_layout()\n",
    "    ax.set_axis_off()\n",
    "    sliced = slice(file, frame, idx, vel_dir, slice_axis)\n",
    "    sliced = abs(HR_slice-sliced)\n",
    "    ax.imshow(sliced, interpolation='nearest', cmap='viridis', origin='lower', vmin=_min, vmax=_max)\n",
    "    \n",
    "    f#ig.savefig(f\"../evaluation/insilico/baseVScombVSens/abs/CardiacM4Dx2/cardiacM4Dx2_{index}\")\n",
    "    index += 1\n",
    "\n",
    "# axis.imshow(LR_slice, interpolation='nearest', cmap='viridis', origin='lower', vmin=0, vmax=1)\n",
    "\n",
    "# for ax, file in zip(axes.flatten(), eval_files):\n",
    "#     sliced = slice(file, frame, idx, vel_dir, slice_axis)\n",
    "#     diff_sliced = abs(HR_slice - sliced)\n",
    "#     _min, _max = min(_min, np.amin(diff_sliced)), max(_max, np.amax(diff_sliced))\n",
    "# test = []\n",
    "# # for ax, file in zip(axes.flatten(), eval_files):\n",
    "#     #ax.set_axis_off()\n",
    "# file = eval_files[0]\n",
    "# sliced = slice(file, frame, idx, vel_dir, slice_axis)\n",
    "# diff_sliced = abs(HR_slice - sliced)\n",
    "# test = axis.imshow(diff_sliced, interpolation='nearest', cmap='viridis', origin='lower', vmin=_min, vmax=_max)\n",
    "\n",
    "#fig.colorbar(test, ax=axes[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_mask = morphology.binary_erosion(binary_mask)\n",
    "boundary_mask = binary_mask - core_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute error GIF, across slice axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3)\n",
    "print(axes)\n",
    "delim = \"results/\"\n",
    "stop_delim = len(test_file) +1\n",
    "start = eval_files[1].find(delim) + len(delim)\n",
    "LR_slice = slice(LR_file, frame, int(idx/2), vel_dir, slice_axis)\n",
    "_min, _max = np.amin(LR_slice), np.amax(LR_slice)\n",
    "print(_min, _max)\n",
    "\n",
    "HR_slice = slice(HR_file, frame, idx, vel_dir, slice_axis)\n",
    "_min, _max = min(_min, np.amin(HR_slice)), max(_max, np.amax(HR_slice))\n",
    "print(_min, _max)\n",
    "skip = 0\n",
    "\n",
    "# Check max and min to set boundaries\n",
    "\n",
    "for ax, file in zip(axes.flatten(), eval_files):\n",
    "    if skip:\n",
    "        skip -= 1\n",
    "        continue\n",
    "    sliced = slice(file, frame, idx, vel_dir, slice_axis)\n",
    "    _min, _max = min(_min, np.amin(sliced)), max(_max, np.amax(sliced))\n",
    "    print(file,_min, _max)\n",
    "\n",
    "\n",
    "\n",
    "skip = 0\n",
    "#axes[0].set_title(\"HR\")\n",
    "axes[0].set_axis_off()\n",
    "axes[0].imshow(LR_slice, interpolation='nearest', cmap='viridis', origin='lower', vmin=_min, vmax=_max)\n",
    "axes[1].set_axis_off()\n",
    "axes[1].imshow(HR_slice, interpolation='nearest', cmap='viridis', origin='lower', vmin=_min, vmax=_max)\n",
    "test = []\n",
    "for ax, file in zip(axes.flatten(), eval_files):\n",
    "    if skip:\n",
    "        skip -= 1\n",
    "        continue\n",
    "    #ax.set_title(file[start:-stop_delim])\n",
    "    ax.set_axis_off()\n",
    "    sliced = slice(file, frame, idx, vel_dir, slice_axis)\n",
    "    test = ax.imshow(sliced, interpolation='nearest', cmap='viridis', origin='lower')#, vmin=_min, vmax=_max)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.set_axis_off()\n",
    "# ax.set_title(\"LR\")\n",
    "# test = ax.imshow(LR_slice, interpolation='nearest', cmap='viridis', origin='lower', vmin=_min, vmax=_max)\n",
    "fig.tight_layout()\n",
    "#fig.colorbar(test, ax=axes[4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot regression plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def crop(hr, pred):\n",
    "    # We assume that if there is a mismatch it's because SR is smaller than HR.\n",
    "    crop = np.asarray(hr.shape) - np.asarray(pred.shape)\n",
    "    hr = hr[crop[0]//2:-crop[0]//2,:,:] if crop[0] else hr\n",
    "    hr = hr[:, crop[1]//2:-crop[1]//2,:] if crop[1] else hr\n",
    "    hr = hr[:, :, crop[2]//2:-crop[2]//2] if crop[2] else hr\n",
    "    return hr\n",
    "\n",
    "def _reg_stats(subplot, hr_vals, sr_vals, color, x, y):\n",
    "    x_text = x\n",
    "    reg = stats.linregress(hr_vals, sr_vals)\n",
    "    x = np.array([-10, 10]) # Start, End point for the regression slope lines\n",
    "    if reg.intercept < 0.0:\n",
    "        reg_stats = f'$y = {reg.slope:.3f}x {reg.intercept:.3f}$\\n$r^2 = {reg.rvalue**2:.3f}$'\n",
    "    else:\n",
    "        reg_stats = f'$y = {reg.slope:.3f}x + {reg.intercept:.3f}$\\n$r^2 = {reg.rvalue**2:.3f}$'\n",
    "    subplot.plot(x, reg.intercept + reg.slope*x, color=color, linestyle='--', alpha=0.3)\n",
    "    subplot.text(x_text, y, reg_stats, horizontalalignment='center', verticalalignment='center', fontsize=7, color=color)\n",
    "\n",
    "def _setup_lin_reg_plot(fig_nr, xlim, ylim, title):\n",
    "    plt.figure(fig_nr)\n",
    "    fig, subplots = plt.subplots(1,3,sharey=True, figsize=plt.figaspect(1/3))\n",
    "    dimensions = [\"x\", \"y\", \"z\"]\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    for i, subplot in enumerate(subplots):\n",
    "        subplot.set_xlim(-xlim,xlim); subplot.set_ylim(-ylim,ylim); subplot.set_xticks([-xlim, xlim]); subplot.set_yticks([-ylim, ylim])\n",
    "        subplot.set_title(f\"$V_{{{dimensions[i]}}}$\"); subplot.set_xlabel(\"$V_{HR}$ [m/s]\"); subplot.set_ylabel(\"$V_{SR}$ [m/s]\")\n",
    "    \n",
    "    return fig, subplots\n",
    "\n",
    "\n",
    "def _plot_data(subplot, hr_vals, sr_vals, color, size, label):\n",
    "    subplot.scatter(hr_vals, sr_vals, s=size, c=[color], label=label)\n",
    "    subplot.legend(loc='lower right', fontsize=7)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def _sample_hrsr(ground_truth_file, prediction_file, mask, peak_flow_idx, samples):\n",
    "    # Use mask to find interesting samples\n",
    "    sample_pot = np.where(mask == 1)\n",
    "    rng = np.random.default_rng()\n",
    "    # Sample <ratio> samples\n",
    "    print(\"pot:\", len(sample_pot[0]))\n",
    "    print(\"samples:\", samples)\n",
    "    sample_idx = rng.choice(len(sample_pot[0]), replace=False, size=samples)\n",
    "    \n",
    "    # Get indexes\n",
    "    x_idx = sample_pot[0][sample_idx]\n",
    "    y_idx = sample_pot[1][sample_idx]\n",
    "    z_idx = sample_pot[2][sample_idx]\n",
    "\n",
    "    with h5py.File(prediction_file, mode = 'r' ) as hf:\n",
    "        sr_u = np.asarray(hf['u'][peak_flow_idx])\n",
    "        sr_u_all = sr_u[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        sr_u_samples = sr_u[x_idx, y_idx, z_idx]\n",
    "        sr_v = np.asarray(hf['v'][peak_flow_idx])\n",
    "        sr_v_all = sr_v[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        sr_v_samples = sr_v[x_idx, y_idx, z_idx]\n",
    "        sr_w = np.asarray(hf['w'][peak_flow_idx])\n",
    "        sr_w_all = sr_w[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        sr_w_samples = sr_w[x_idx, y_idx, z_idx]\n",
    "        \n",
    "    with h5py.File(ground_truth_file, mode = 'r' ) as hf:\n",
    "        # Get velocity values in all directions\n",
    "        hr_u = crop(np.asarray(hf['u'][peak_flow_idx]), sr_u)\n",
    "        hr_u_all = hr_u[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        hr_u_samples = hr_u[x_idx, y_idx, z_idx]\n",
    "        hr_v = crop(np.asarray(hf['v'][peak_flow_idx]), sr_v)\n",
    "        hr_v_all = hr_v[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        hr_v_samples = hr_v[x_idx, y_idx, z_idx]\n",
    "        hr_w = crop(np.asarray(hf['w'][peak_flow_idx]), sr_w)\n",
    "        hr_w_all = hr_w[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        hr_w_samples = hr_w[x_idx, y_idx, z_idx]\n",
    "        \n",
    "        \n",
    "    return [hr_u_samples, hr_v_samples, hr_w_samples], [sr_u_samples, sr_v_samples, sr_w_samples], [hr_u_all, hr_v_all, hr_w_all], [sr_u_all, sr_v_all, sr_w_all]\n",
    "    \n",
    "\n",
    "def draw_multi_reg_line(ground_truth_file, prediction_dirs, peak_flow_idx, binary_mask, prediction_filename):\n",
    "    \"\"\" Plot a linear regression between HR and predicted SR in peak flow frame \"\"\"\n",
    "    #\n",
    "    # Parameters\n",
    "    #\n",
    "\n",
    "    # Hard coded\n",
    "    boundary_voxels = 3000\n",
    "    core_voxels = 3032\n",
    "    voxels = 1000\n",
    "\n",
    "    core_fig_nr = 1\n",
    "    boundary_fig_nr = 2\n",
    "    xlim = 1.0\n",
    "    ylim = 1.0\n",
    "\n",
    "    colors = [\"red\", \"green\", \"blue\", \"orange\", \"purple\", \"black\"]\n",
    "    x = -2*xlim/3\n",
    "    y = [4*ylim/5, 3*ylim/5, 2*ylim/5, ylim/5, 0, -ylim/5]\n",
    "    \n",
    "    # core_mask = morphology.binary_erosion(binary_mask)\n",
    "    # boundary_mask = binary_mask - core_mask\n",
    "\n",
    "    model_name_start = prediction_dirs[0].find(\"/\" ,prediction_dirs[0].find(\"/\")+1)+1\n",
    "    \n",
    "    # core_fig, core_subplots = _setup_lin_reg_plot(core_fig_nr, xlim, ylim, f\"Core Voxels - {prediction_filename[:-6]}\")\n",
    "    # boundary_fig, boundary_subplots = _setup_lin_reg_plot(boundary_fig_nr, xlim, ylim, f\"Boundary Voxels - {prediction_filename[:-6]}\")\n",
    "    fig, subplots = _setup_lin_reg_plot(1, xlim, ylim, f\"Voxels - {prediction_filename[:-6]}\")\n",
    "    print(len(subplots))\n",
    "    \n",
    "    print(f\"Plotting regression lines...\")\n",
    "    for i, prediction_dir in enumerate(prediction_dirs):\n",
    "        prediction_file = f\"{prediction_dir}/{prediction_filename}\"\n",
    "        print(prediction_file)\n",
    "        #boundary_hr, boundary_sr = _sample_hrsr(ground_truth_file, prediction_file, boundary_mask, peak_flow_idx, boundary_voxels)\n",
    "        hr_samples, sr_samples, hr, sr = _sample_hrsr(ground_truth_file, prediction_file, binary_mask, peak_flow_idx, voxels)\n",
    "        _plot_data(subplots[0], hr_samples[0], sr_samples[0], colors[i], 0.5, label=prediction_dir[model_name_start:])\n",
    "        _reg_stats(subplots[0], hr[0], sr[0], colors[i], x, y[i])\n",
    "        _plot_data(subplots[1], hr_samples[1], sr_samples[1], colors[i], 0.5, label=prediction_dir[model_name_start:])\n",
    "        _reg_stats(subplots[1], hr[1], sr[1], colors[i], x, y[i])\n",
    "        _plot_data(subplots[2], hr_samples[2], sr_samples[2], colors[i], 0.5, label=prediction_dir[model_name_start:])\n",
    "        _reg_stats(subplots[2], hr[2], sr[2], colors[i], x, y[i])\n",
    "        \n",
    "        # _plot_data(boundary_subplots[0], boundary_hr[0], boundary_sr[0], x, y[i], colors[i], 0.5, label=prediction_dir[model_name_start:])\n",
    "        # _plot_data(boundary_subplots[1], boundary_hr[1], boundary_sr[1], x, y[i], colors[i], 0.5, label=prediction_dir[model_name_start:])\n",
    "        # _plot_data(boundary_subplots[2], boundary_hr[2], boundary_sr[2], x, y[i], colors[i], 0.5, label=prediction_dir[model_name_start:])\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    #fig.savefig(f\"../evaluation/{prediction_filename[:-3]}_aorta_{timestamp}_reg.png\") \n",
    "    #boundary_fig.savefig(f\"../evaluation/{prediction_filename[:-3]}_boundary_{timestamp}_reg.png\")\n",
    "\n",
    "def draw_reg_line(ground_truth_file, prediction_dirs, peak_flow_idx, binary_mask, prediction_filename):\n",
    "    \"\"\" Plot a linear regression between HR and predicted SR in peak flow frame \"\"\"\n",
    "    #\n",
    "    # Parameters\n",
    "    #\n",
    "\n",
    "    # Hard coded\n",
    "    voxels = 2500\n",
    "\n",
    "    core_fig_nr = 1\n",
    "    boundary_fig_nr = 2\n",
    "    xlim = 1.0\n",
    "    ylim = 1.0\n",
    "\n",
    "    colors = [u'#1f77b4', u'#ff7f0e', u'#2ca02c', u'#d62728', u'#9467bd', u'#8c564b']\n",
    "    x = -2*xlim/3\n",
    "    y = [4*ylim/5, 3*ylim/5, 2*ylim/5, ylim/5, 0, -ylim/5]\n",
    "\n",
    "    model_name_start = prediction_dirs[0].find(\"/\" ,prediction_dirs[0].find(\"/\")+1)+1\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(-xlim,xlim); ax.set_ylim(-ylim,ylim); ax.set_xticks([-xlim, xlim]); ax.set_yticks([-ylim, ylim])\n",
    "    ax.set_xlabel(\"$V_{HR}$ [m/s]\"); #subplot.set_ylabel(\"$V_{SR}$ [m/s]\"); ax.set_title(f\"$V_{{{dimensions[i]}}}$\"); \n",
    "    \n",
    "    \n",
    "    #boundary_fig, boundary_subplots = _setup_lin_reg_plot(boundary_fig_nr, xlim, ylim, f\"Boundary Voxels - {prediction_filename[:-6]}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\"Plotting regression lines...\")\n",
    "    for i, prediction_dir in enumerate(prediction_dirs):\n",
    "        prediction_file = f\"{prediction_dir}/{prediction_filename}\"\n",
    "        hr, sr = _sample_hrsr(ground_truth_file, prediction_file, binary_mask, peak_flow_idx, voxels)\n",
    "        #core_hr, core_sr = _sample_hrsr(ground_truth_file, prediction_file, core_mask, peak_flow_idx, core_voxels)\n",
    "        _plot_data(ax, hr[0], sr[0], x, y[i], colors[i], 0.5, label=prediction_dir[model_name_start:])\n",
    "    #core_fig.savefig(f\"../evaluation/{prediction_filename[:-3]}_core_{timestamp}_reg.png\") \n",
    "    #boundary_fig.savefig(f\"../evaluation/{prediction_filename[:-3]}_boundary_{timestamp}_reg.png\") \n",
    "     \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(eval_files[0], mode = 'r') as hf:\n",
    "    u = tf.convert_to_tensor(hf['u'][0])\n",
    "\n",
    "with h5py.File(HR_file, 'r') as hf:\n",
    "    mask = tf.convert_to_tensor(hf['mask'])\n",
    "    if len(mask.shape) == 3: \n",
    "        mask = crop(mask, u)[tf.newaxis]\n",
    "    else:\n",
    "        mask = crop(mask[frame], u)[tf.newaxis]\n",
    "    # Casting excessively because eager tensors won't dynamically cast. \n",
    "    binary_mask = tf.cast((tf.cast(mask, dtype=tf.float32) >= mask_threshold), dtype=tf.float32)\n",
    "    data_count = len(hf.get(\"u\"))\n",
    "        \n",
    "print(HR_file, test_file, frame, eval_dirs)\n",
    "draw_multi_reg_line(HR_file, eval_dirs, frame, tf.squeeze(binary_mask, axis=[0]), test_file)\n",
    "plt.show()\n",
    "#draw_reg_line(HR_file, eval_dirs, frame, tf.squeeze(binary_mask, axis=[0]), test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def crop(hr, pred):\n",
    "    # We assume that if there is a mismatch it's because SR is smaller than HR.\n",
    "    crop = np.asarray(hr.shape) - np.asarray(pred.shape)\n",
    "    hr = hr[crop[0]//2:-crop[0]//2,:,:] if crop[0] else hr\n",
    "    hr = hr[:, crop[1]//2:-crop[1]//2,:] if crop[1] else hr\n",
    "    hr = hr[:, :, crop[2]//2:-crop[2]//2] if crop[2] else hr\n",
    "    return hr\n",
    "\n",
    "def _reg_stats(subplot, hr_vals, sr_vals, color, x, y):\n",
    "    x_text = x\n",
    "    reg = stats.linregress(hr_vals, sr_vals)\n",
    "    x = np.array([-10, 10]) # Start, End point for the regression slope lines\n",
    "    if reg.intercept < 0.0:\n",
    "        reg_stats = f'$y = {reg.slope:.3f}x {reg.intercept:.3f}$\\n$r^2 = {reg.rvalue**2:.3f}$'\n",
    "    else:\n",
    "        reg_stats = f'$y = {reg.slope:.3f}x + {reg.intercept:.3f}$\\n$r^2 = {reg.rvalue**2:.3f}$'\n",
    "    subplot.plot(x, reg.intercept + reg.slope*x, color=color, linestyle='--', alpha=0.3)\n",
    "    subplot.text(x_text, y, reg_stats, horizontalalignment='center', verticalalignment='center', fontsize=9, color=color)\n",
    "\n",
    "def _setup_lin_reg_plot(fig_nr, xlim, ylim):\n",
    "    plt.figure(fig_nr)\n",
    "    fig, subplots = plt.subplots(1,3,sharey=True, figsize=plt.figaspect(1/3))\n",
    "    dimensions = [\"x\", \"y\", \"z\"]\n",
    "    for i, subplot in enumerate(subplots):\n",
    "        subplot.set_xlim(-xlim,xlim)\n",
    "        subplot.set_ylim(-ylim,ylim)\n",
    "        subplot.set_xticks([])\n",
    "        \n",
    "        subplot.set_yticks([-ylim, ylim])\n",
    "        #subplot.set_title(f\"$V_{{{dimensions[i]}}}$\"); \n",
    "        subplot.set_xlabel(\"$V_{HR}$ [m/s]\"); subplot.set_xticks([-xlim,xlim])\n",
    "        if i == 0:\n",
    "            subplot.set_ylabel(\"$V_{SR}$ [m/s]\")\n",
    "    \n",
    "    return fig, subplots\n",
    "\n",
    "\n",
    "def _plot_data(subplot, hr_vals, sr_vals, color, marker, size, label):\n",
    "    subplot.scatter(hr_vals, sr_vals, s=size, c=[color], label=label, marker=marker)\n",
    "    #subplot.legend(loc='lower right', fontsize=7)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def _sample_hrsr(ground_truth_file, prediction_file, mask, peak_flow_idx, samples):\n",
    "    # Use mask to find interesting samples\n",
    "    sample_pot = np.where(mask == 1)\n",
    "    rng = np.random.default_rng()\n",
    "    # Sample <ratio> samples\n",
    "    print(\"pot:\", len(sample_pot[0]))\n",
    "    print(\"samples:\", samples)\n",
    "    sample_idx = rng.choice(len(sample_pot[0]), replace=False, size=samples)\n",
    "    \n",
    "    # Get indexes\n",
    "    x_idx = np.array_split(sample_pot[0][sample_idx], 3)\n",
    "    y_idx = np.array_split(sample_pot[1][sample_idx], 3)\n",
    "    z_idx = np.array_split(sample_pot[2][sample_idx], 3)\n",
    "\n",
    "    with h5py.File(prediction_file, mode = 'r' ) as hf:\n",
    "        sr_u = np.asarray(hf['u'][peak_flow_idx])\n",
    "        sr_u_all = sr_u[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        sr_u_samples = sr_u[x_idx[0], y_idx[0], z_idx[0]]\n",
    "\n",
    "        sr_v = np.asarray(hf['v'][peak_flow_idx])\n",
    "        sr_v_all = sr_v[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        sr_v_samples = sr_v[x_idx[1], y_idx[1], z_idx[1]]\n",
    "\n",
    "        sr_w = np.asarray(hf['w'][peak_flow_idx])\n",
    "        sr_w_all = sr_w[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        sr_w_samples = sr_w[x_idx[2], y_idx[2], z_idx[2]]\n",
    "        \n",
    "    with h5py.File(ground_truth_file, mode = 'r' ) as hf:\n",
    "        # Get velocity values in all directions\n",
    "        hr_u = crop(np.asarray(hf['u'][peak_flow_idx]), sr_u)\n",
    "        hr_u_all = hr_u[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        hr_u_samples = hr_u[x_idx[0], y_idx[0], z_idx[0]]\n",
    "\n",
    "        hr_v = crop(np.asarray(hf['v'][peak_flow_idx]), sr_v)\n",
    "        hr_v_all = hr_v[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        hr_v_samples = hr_v[x_idx[1], y_idx[1], z_idx[1]]\n",
    "        \n",
    "        hr_w = crop(np.asarray(hf['w'][peak_flow_idx]), sr_w)\n",
    "        hr_w_all = hr_w[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        hr_w_samples = hr_w[x_idx[2], y_idx[2], z_idx[2]]\n",
    "        \n",
    "        \n",
    "    return np.concatenate((hr_u_samples, hr_v_samples, hr_w_samples), axis=None), np.concatenate((sr_u_samples, sr_v_samples, sr_w_samples), axis=None), np.concatenate((hr_u_all, hr_v_all, hr_w_all), axis=None), np.concatenate((sr_u_all, sr_v_all, sr_w_all), axis=None)\n",
    "    \n",
    "\n",
    "def draw_multi_reg_line(ground_truth_file, prediction_dirs, peak_flow_idx, binary_mask, prediction_filename, subplot):\n",
    "    \"\"\" Plot a linear regression between HR and predicted SR in peak flow frame \"\"\"\n",
    "    #\n",
    "    # Parameters\n",
    "    #\n",
    "\n",
    "    # Hard coded\n",
    "    voxels = 5000\n",
    "    xlim = 1.0\n",
    "    ylim = 1.0\n",
    "    viridis = cm.get_cmap('viridis', 12)\n",
    "    inferno = cm.get_cmap('inferno', 12)\n",
    "    plasma = cm.get_cmap('plasma', 12)\n",
    "\n",
    "    # colors = [viridis(0.9), viridis(0.5), inferno(0.5), viridis(0.), \"orange\", \"purple\", \"black\"]\n",
    "    colors = [inferno(0.),inferno(0.5), viridis(0.3), viridis(0.6), \"orange\", \"purple\", \"black\"]\n",
    "    markers = ['.', '1', '*', '+']\n",
    "    x = -2*xlim/4\n",
    "    y = [4*ylim/5, 3*ylim/5, 2*ylim/5, ylim/5, 0, -ylim/5]\n",
    "    \n",
    "\n",
    "    model_name_start = prediction_dirs[0].find(\"/\" ,prediction_dirs[0].find(\"/\")+1)+1\n",
    "    \n",
    "    print(f\"Plotting regression lines...\")\n",
    "    for i, prediction_dir in enumerate(prediction_dirs):\n",
    "        prediction_file = f\"{prediction_dir}/{prediction_filename}\"\n",
    "        print(prediction_file)\n",
    "        hr_samples, sr_samples, hr, sr = _sample_hrsr(ground_truth_file, prediction_file, binary_mask, peak_flow_idx, voxels)\n",
    "        _plot_data(subplot, hr_samples, sr_samples, colors[i], markers[i], 0.5, label=f\"4DFlowNet-{prediction_dir[model_name_start:]}\")\n",
    "        _reg_stats(subplot, hr, sr, colors[i], x, y[i])\n",
    "        \n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    #fig.savefig(f\"../evaluation/{prediction_filename[:-3]}_aorta_{timestamp}_reg.png\") \n",
    "    #boundary_fig.savefig(f\"../evaluation/{prediction_filename[:-3]}_boundary_{timestamp}_reg.png\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = 1.0\n",
    "ylim = 1.0\n",
    "fig, subplots = _setup_lin_reg_plot(1, xlim, ylim)\n",
    "i = 0\n",
    "for HR_ in HR_files:\n",
    "    with h5py.File(eval_files_for_all_HR[i][0], mode = 'r') as hf:\n",
    "        u = tf.convert_to_tensor(hf['u'][0])\n",
    "\n",
    "    with h5py.File(HR_, 'r') as hf:\n",
    "        mask = tf.convert_to_tensor(hf['mask'])\n",
    "        # Casting excessively because eager tensors won't dynamically cast. \n",
    "        binary_mask = tf.cast((tf.cast(mask, dtype=tf.float32) >= mask_threshold), dtype=tf.float32)\n",
    "        data_count = len(hf.get(\"u\"))\n",
    "            \n",
    "    print(HR_file, test_file, frame, eval_dirs)\n",
    "    draw_multi_reg_line(HR_, eval_dirs, test_frames[i], binary_mask, test_files[i], subplots[i])\n",
    "    i += 1\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"../evaluation/insilico/NumberOfLearners/Regressions/bagging_reg.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefficients(ground_truth_file, prediction_file, mask, peak_flow_idx):\n",
    "    \n",
    "    # Use mask to find interesting samples\n",
    "    sample_pot = np.where(mask == 1)\n",
    "\n",
    "    with h5py.File(prediction_file, mode = 'r' ) as hf:\n",
    "        sr_u = np.asarray(hf['u'][peak_flow_idx])\n",
    "        sr_u_all = sr_u[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "\n",
    "        sr_v = np.asarray(hf['v'][peak_flow_idx])\n",
    "        sr_v_all = sr_v[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "\n",
    "        sr_w = np.asarray(hf['w'][peak_flow_idx])\n",
    "        sr_w_all = sr_w[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        \n",
    "    with h5py.File(ground_truth_file, mode = 'r' ) as hf:\n",
    "        # Get velocity values in all directions\n",
    "        hr_u = crop(np.asarray(hf['u'][peak_flow_idx]), sr_u)\n",
    "        hr_u_all = hr_u[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        hr_v = crop(np.asarray(hf['v'][peak_flow_idx]), sr_v)\n",
    "        hr_v_all = hr_v[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        hr_w = crop(np.asarray(hf['w'][peak_flow_idx]), sr_w)\n",
    "        hr_w_all = hr_w[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        \n",
    "        \n",
    "    return [hr_u_all, hr_v_all, hr_w_all], [sr_u_all, sr_v_all, sr_w_all]\n",
    "    \n",
    "def get_reg_stats(hr_vals, sr_vals):\n",
    "    slope = []\n",
    "    rsquared = []\n",
    "    for hr, sr in zip(hr_vals, sr_vals):\n",
    "        reg = stats.linregress(hr, sr)\n",
    "        slope.append(np.round(reg.slope,3))\n",
    "        rsquared.append(np.round(reg.rvalue**2, 3))\n",
    "    return tuple(slope), tuple(rsquared)\n",
    "\n",
    "xlim = 1.0\n",
    "ylim = 1.0\n",
    "i = 0\n",
    "for HR_ in HR_files:\n",
    "    with h5py.File(eval_files_for_all_HR[i][0], mode = 'r') as hf:\n",
    "        u = tf.convert_to_tensor(hf['u'][0])\n",
    "\n",
    "    with h5py.File(HR_, 'r') as hf:\n",
    "        mask = tf.convert_to_tensor(hf['mask'])\n",
    "        # Casting excessively because eager tensors won't dynamically cast. \n",
    "        binary_mask = tf.cast((tf.cast(mask, dtype=tf.float32) >= mask_threshold), dtype=tf.float32)\n",
    "        data_count = len(hf.get(\"u\"))\n",
    "            \n",
    "    print(\"Ground Truth file:\", HR_)\n",
    "    for prediction_dir in eval_dirs:\n",
    "        prediction_file = f\"{prediction_dir}/{test_files[i]}\"\n",
    "        print(\"Prediction file:\",  prediction_file)\n",
    "        hr, sr = get_coefficients(HR_,prediction_file,binary_mask,test_frames[i])\n",
    "        slope, rsquared = get_reg_stats(hr, sr)\n",
    "        print(\"Slope:\", slope)\n",
    "        print(\"R^2:\", rsquared)\n",
    "\n",
    "    \n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-values and R^2-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"cardiac\"\n",
    "#model = \"Bagging-12\" # Best Bagging\n",
    "model = \"Stacking-3-archblock\" # Best stacking\n",
    "\n",
    "if domain == \"aorta\":\n",
    "    predictions = [f\"../results/{model}/P01_fft_SR.h5\",\n",
    "               f\"../results/{model}/P02_fft_SR.h5\",\n",
    "               f\"../results/{model}/P03_fft_SR.h5\",\n",
    "               f\"../results/{model}/P04_fft_SR.h5\",\n",
    "               f\"../results/{model}/P05_fft_SR.h5\",]\n",
    "    HR = [\"../THESIS_INVIVO_DATA/THORAX/P01/h5/P01_aorta_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/THORAX/P02/h5/P02_aorta_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/THORAX/P03/h5/P03_aorta_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/THORAX/P04/h5/P04_aorta_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/THORAX/P05/h5/P05_aorta_HR.h5\",]\n",
    "    names = [\"P01\", \"P02\", \"P03\", \"P04\", \"P05\"]\n",
    "    peak_frames = [4, 5, 1, 6, 5]\n",
    "    frames = 25\n",
    "elif domain == \"cardiac\":\n",
    "    predictions = [f\"../results/{model}/P01_fft_SR.h5\",\n",
    "               f\"../results/{model}/P02_fft_SR.h5\",\n",
    "               f\"../results/{model}/P03_fft_SR.h5\",\n",
    "               f\"../results/{model}/P04_fft_SR.h5\",\n",
    "               f\"../results/{model}/P05_fft_SR.h5\",]\n",
    "    HR = [\"../THESIS_INVIVO_DATA/THORAX/P01/h5/P01_cardiac_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/THORAX/P02/h5/P02_cardiac_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/THORAX/P03/h5/P03_cardiac_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/THORAX/P04/h5/P04_cardiac_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/THORAX/P05/h5/P05_cardiac_HR.h5\",]\n",
    "    names = [\"P01\", \"P02\", \"P03\", \"P04\", \"P05\"]\n",
    "    peak_frames = [15, 15, 13, 17, 15]\n",
    "    frames = 25\n",
    "elif domain == \"cerebro\":\n",
    "    predictions = [f\"../results/{model}/HV01_fft_SR.h5\",\n",
    "               f\"../results/{model}/HV03_fft_SR.h5\",\n",
    "               f\"../results/{model}/ICAD28_fft_SR.h5\",\n",
    "               f\"../results/{model}/ICAD48_fft_SR.h5\",\n",
    "               f\"../results/{model}/ICAD33_fft_SR.h5\",]\n",
    "    HR = [\"../THESIS_INVIVO_DATA/BRAIN/Healthy/HV01/h5/HV01_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/BRAIN/Healthy/HV03/h5/HV03_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/BRAIN/Mild/ICAD28/h5/ICAD28_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/BRAIN/Moderate/ICAD48/h5/ICAD48_HR.h5\",\n",
    "               \"../THESIS_INVIVO_DATA/BRAIN/Severe/ICAD33/h5/ICAD33_HR.h5\",]\n",
    "    names = [\"HV01\", \"HV03\", \"ICAD28\", \"ICAD48\", \"ICAD33\"]\n",
    "    peak_frames = [1, 5, 0, 1, 1]\n",
    "    frames = 25\n",
    "\n",
    "def crop(hr, pred):\n",
    "    # We assume that if there is a mismatch it's because SR is smaller than HR.\n",
    "    crop = np.asarray(hr.shape) - np.asarray(pred.shape)\n",
    "    hr = hr[crop[0]//2:-crop[0]//2,:,:] if crop[0] else hr\n",
    "    hr = hr[:, crop[1]//2:-crop[1]//2,:] if crop[1] else hr\n",
    "    hr = hr[:, :, crop[2]//2:-crop[2]//2] if crop[2] else hr\n",
    "    return hr\n",
    "\n",
    "def _get_hrsr(ground_truth_file, prediction_file, mask, idx):\n",
    "    # Use mask to find interesting samples\n",
    "    sample_pot = np.where(mask == 1)\n",
    "    # Sample <ratio> samples\n",
    "    \n",
    "    \n",
    "\n",
    "    with h5py.File(prediction_file, mode = 'r' ) as hf:\n",
    "        sr_u = np.asarray(hf['u'][idx])\n",
    "        sr_u_all = sr_u[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "\n",
    "        sr_v = np.asarray(hf['v'][idx])\n",
    "        sr_v_all = sr_v[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "\n",
    "        sr_w = np.asarray(hf['w'][idx])\n",
    "        sr_w_all = sr_w[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        \n",
    "    with h5py.File(ground_truth_file, mode = 'r' ) as hf:\n",
    "        # Get velocity values in all directions\n",
    "        hr_u = crop(np.asarray(hf['u'][idx]), sr_u)\n",
    "        hr_u_all = hr_u[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "\n",
    "        hr_v = crop(np.asarray(hf['v'][idx]), sr_v)\n",
    "        hr_v_all = hr_v[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        \n",
    "        hr_w = crop(np.asarray(hf['w'][idx]), sr_w)\n",
    "        hr_w_all = hr_w[sample_pot[0], sample_pot[1], sample_pot[2]]\n",
    "        \n",
    "        \n",
    "    return np.concatenate((hr_u_all, hr_v_all, hr_w_all), axis=None), np.concatenate((sr_u_all, sr_v_all, sr_w_all), axis=None)\n",
    "\n",
    "fig1, sub1 = plt.subplots()\n",
    "fig2, sub2 = plt.subplots()\n",
    "k = 0\n",
    "for prediction, gt, peak in zip(predictions, HR, peak_frames):\n",
    "    coeff = []\n",
    "    r = []\n",
    "    with h5py.File(gt, mode = 'r' ) as hf:\n",
    "            u = np.asarray(hf['u'])\n",
    "            frames = u.shape[0]\n",
    "    for i in range(frames):\n",
    "        with h5py.File(gt, mode = 'r' ) as hf:\n",
    "            mask = np.asarray(hf['mask'])\n",
    "            if len(mask.shape) == 4 and mask.shape[0] != 1:\n",
    "                mask = mask[i]\n",
    "            elif len(mask.shape) == 4:\n",
    "                mask = mask[0]\n",
    "        hr, sr = _get_hrsr(gt, prediction, mask, i)\n",
    "        reg = stats.linregress(hr, sr)\n",
    "        coeff.append(np.round(reg.slope, 3))\n",
    "        r.append(np.round(reg.rvalue**2, 3))\n",
    "    print(\"Subject:\", gt)\n",
    "    print(\"Peak K:\", \"{:.3f}\".format(np.round(coeff[peak], 3)))\n",
    "    print(\"Peak R^2:\", \"{:.3f}\".format(np.round(r[peak], 3)))\n",
    "    print(\"Average K:\", \"{:.3f}\".format(np.round(np.mean(coeff), 3)), \"+\", \"{:.3f}\".format(np.round(np.std(coeff), 3)))\n",
    "    print(\"Average R^2:\", \"{:.3f}\".format(np.round(np.mean(r), 3)), \"+\", \"{:.3f}\".format(np.round(np.std(r), 3)))\n",
    "    sub1.plot(coeff, label=names[k])\n",
    "    sub2.plot(r, label=names[k])\n",
    "    k += 1\n",
    "sub1.set_title(\"K-value\")\n",
    "sub2.set_title(\"R^2-value\")\n",
    "sub1.legend()\n",
    "sub2.legend()\n",
    "plt.show()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,4,7])\n",
    "np.mean(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20e49a7e9319e525fa1e72e3d22374bf9d2d3735a43c22e1dbaebf8b67ddf8f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
