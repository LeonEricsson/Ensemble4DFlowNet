{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning - Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Network import loss_utils\n",
    "from utils import evaluation_utils as e_utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_FOLDS = 3\n",
    "NUM_BASE_MODELS = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training & validation data for base models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach #1: K-folds on subset of train + validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "data_dir = '../data'\n",
    "save_dir = '../data/stacking_3test'\n",
    "\n",
    "# Training patch files\n",
    "train_patches = [\"aortaTrain_patches.csv\", \"cerebTrain_patches.csv\", \"cardiacTrain_patches.csv\"]\n",
    "training_files = [f'{data_dir}/{file}' for file in train_patches]\n",
    "\n",
    "# Validation patch files\n",
    "val_patches = [\"aortaVal_patches.csv\", \"cerebVal_patches.csv\", \"cardiacVal_patches.csv\"]\n",
    "validate_files = [f'{data_dir}/{file}' for file in val_patches]\n",
    "\n",
    "# CSV Header file\n",
    "header_file = f'{data_dir}/header.csv'\n",
    "\n",
    "# Parameters\n",
    "sampling_fraction = 0.15\n",
    "\n",
    "# Create save directory\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Load data file and indexes\n",
    "base_trainset = [np.genfromtxt(file, delimiter=',', skip_header=True, dtype='unicode') for file in training_files]\n",
    "base_valset = [np.genfromtxt(file, delimiter=',', skip_header=True, dtype='unicode') for file in validate_files]\n",
    "\n",
    "header = ','.join(list(np.genfromtxt(header_file, delimiter=',', dtype='unicode')))\n",
    "                  \n",
    "# Random sampling\n",
    "train_indices = [np.random.choice(comp.shape[0], int(comp.shape[0]*sampling_fraction), replace=False) for comp in base_trainset]\n",
    "val_indices = [np.random.choice(comp.shape[0], int(comp.shape[0]*sampling_fraction), replace=False) for comp in base_valset]\n",
    "\n",
    "# Slice sample \n",
    "base_trainset = [[data[indices] for j, indices in enumerate(train_indices) if j == i] for i, data in enumerate(base_trainset)]\n",
    "base_valset = [[data[indices] for j, indices in enumerate(val_indices) if j == i] for i, data in enumerate(base_valset)]\n",
    "\n",
    "base_trainset = np.concatenate([comp[0] for comp in base_trainset], axis=0)\n",
    "base_valset = np.concatenate([comp[0] for comp in base_valset], axis=0)\n",
    "\n",
    "np.random.shuffle(base_trainset) \n",
    "np.random.shuffle(base_valset) \n",
    "\n",
    "train_folds = np.array_split(base_trainset, K_FOLDS)\n",
    "val_folds = np.array_split(base_valset, K_FOLDS)\n",
    "\n",
    "# Save training folds\n",
    "for k, fold in enumerate(train_folds):\n",
    "    trainset_meta = fold\n",
    "    trainset_base = np.concatenate([f for j, f in enumerate(train_folds) if j != k])\n",
    "    \n",
    "    output_meta = f'{save_dir}/fold{k}_meta_patches.csv'\n",
    "    output_base = f'{save_dir}/fold{k}_base_patches.csv'\n",
    "    np.savetxt(output_meta, trainset_meta,fmt='%s',delimiter=',', comments='', header=header)\n",
    "    np.savetxt(output_base, trainset_base,fmt='%s',delimiter=',', comments='',header=header)\n",
    "\n",
    "# Save validation folds\n",
    "for k, fold in enumerate(val_folds):\n",
    "    np.savetxt(f'{save_dir}/fold{k}_val_patches.csv', fold, fmt='%s',delimiter=',', comments='', header=header)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach #2: K-folds on entire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "data_dir = '../data'\n",
    "save_dir = '../data/stacking_small'\n",
    "training_file = '{}/train_patches.csv'.format(data_dir)\n",
    "validation_file = '{}/val_patches.csv'.format(data_dir)\n",
    "\n",
    "# Create save directory\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "# Load data file and indexes\n",
    "base_trainset = np.genfromtxt(training_file, delimiter=',', dtype='unicode')\n",
    "base_valset = np.genfromtxt(validation_file, delimiter=',', dtype='unicode')\n",
    "\n",
    "# Save header\n",
    "header = ','.join(list(base_trainset[0,:]))\n",
    "base_trainset = base_trainset[1:, :]\n",
    "\n",
    "np.random.shuffle(base_trainset) \n",
    "np.random.shuffle(base_valset) \n",
    "\n",
    "train_folds = np.array_split(base_trainset, K_FOLDS)\n",
    "val_folds = np.array_split(base_valset, K_FOLDS)\n",
    "\n",
    "# Save training folds\n",
    "for k, fold in enumerate(train_folds):\n",
    "    trainset_meta = fold\n",
    "    trainset_base = np.concatenate([f for j, f in enumerate(train_folds) if j != k])\n",
    "    \n",
    "    output_meta = f'{save_dir}/fold{k}_meta_patches.csv'\n",
    "    output_base = f'{save_dir}/fold{k}_base_patches.csv'\n",
    "    np.savetxt(output_meta, trainset_meta,fmt='%s',delimiter=',', comments='', header=header)\n",
    "    np.savetxt(output_base, trainset_base,fmt='%s',delimiter=',', comments='',header=header)\n",
    "   \n",
    "# Save validation folds \n",
    "for k, valfold in enumerate(val_folds):\n",
    "    np.savetxt(f'{save_dir}/fold{k}_val_patches.csv', valfold, fmt='%s',delimiter=',', comments='', header=header)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach #3: K training splits and 1 meta split over entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "data_dir = '../data'\n",
    "save_dir = '../data/stacking_2splits'\n",
    "training_file = '{}/train_patches.csv'.format(data_dir)\n",
    "validation_file = '{}/val_patches.csv'.format(data_dir)\n",
    "\n",
    "# Validation patch files\n",
    "val_patches = [\"aortaVal_patches.csv\", \"cerebVal_patches.csv\", \"cardiacVal_patches.csv\"]\n",
    "validate_files = [f'{data_dir}/{file}' for file in val_patches]\n",
    "\n",
    "# Base learner splits\n",
    "K_SPLITS = 2\n",
    "\n",
    "# Number of samples to save/put aside for meta learner\n",
    "meta_train_samples = 8e3\n",
    "meta_val_samples = 2.5e3\n",
    "\n",
    "# Create save directory\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "# Prepare data - Base Learner   \n",
    "    \n",
    "# Load data file and indexes\n",
    "base_trainset = np.genfromtxt(training_file, delimiter=',', dtype='unicode')\n",
    "base_valset = np.genfromtxt(validation_file, delimiter=',', skip_header=True, dtype='unicode')\n",
    "\n",
    "# Save header\n",
    "header = ','.join(list(base_trainset[0,:]))\n",
    "base_trainset = base_trainset[1:, :]\n",
    "\n",
    "np.random.shuffle(base_trainset) \n",
    "np.random.shuffle(base_valset) \n",
    "\n",
    "base_train_splits = np.array_split(base_trainset, K_SPLITS)\n",
    "base_val_splits = np.array_split(base_valset, K_SPLITS)\n",
    "\n",
    "# Prepare data - Meta Learner\n",
    "\n",
    "# Load data file and indexes\n",
    "meta_data = [np.genfromtxt(file, delimiter=',', skip_header=True, dtype='unicode') for file in validate_files]\n",
    "\n",
    "meta_train = []\n",
    "meta_val = []\n",
    "\n",
    "for compartment_data in meta_data:\n",
    "    c_splits = np.array_split(compartment_data, 3)\n",
    "    c_splits = [np.concatenate([c_splits[0], c_splits[1]]), c_splits[2]] # Join the first 2 splits\n",
    "    \n",
    "    # Sample meta data\n",
    "    train_indices = np.random.choice(len(c_splits[0]), int(meta_train_samples/len(meta_data)), replace=False)\n",
    "    val_indices = np.random.choice(len(c_splits[1]), int(meta_val_samples/len(meta_data)), replace=False)\n",
    "    \n",
    "    # Extract indices for meta training and validation\n",
    "    c_train = c_splits[0][train_indices]\n",
    "    c_val = c_splits[1][val_indices]\n",
    "    \n",
    "    meta_train += c_train.tolist()\n",
    "    meta_val += c_val.tolist()\n",
    "\n",
    "# Save base learner training splits\n",
    "for k, t_split in enumerate(base_train_splits):\n",
    "    name = f'{save_dir}/split{k}_base_patches.csv'\n",
    "    np.savetxt(name, t_split, fmt='%s',delimiter=',', comments='', header=header)\n",
    "   \n",
    "# Save base learner validation splits \n",
    "for k, v_split in enumerate(base_val_splits):\n",
    "    name = f'{save_dir}/split{k}_val_patches.csv'\n",
    "    np.savetxt(name, v_split, fmt='%s',delimiter=',', comments='', header=header)\n",
    "\n",
    "# Save meta learner data\n",
    "np.savetxt(f'{save_dir}/meta_train_patches.csv', meta_train, fmt='%s',delimiter=',', comments='', header=header)\n",
    "np.savetxt(f'{save_dir}/meta_val_patches.csv', meta_val, fmt='%s',delimiter=',', comments='', header=header)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach #4: Compartment-wise splits with extracted meta split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "save_dir = '../data/stacking_compartment'\n",
    "\n",
    "# Training patch files\n",
    "train_patches = [\"aortaTrain_patches.csv\", \"cerebTrain_patches.csv\", \"cardiacTrain_patches.csv\"]\n",
    "training_files = [f'{data_dir}/{file}' for file in train_patches]\n",
    "\n",
    "# Validation patch files\n",
    "val_patches = [\"aortaVal_patches.csv\", \"cerebVal_patches.csv\", \"cardiacVal_patches.csv\"]\n",
    "validate_files = [f'{data_dir}/{file}' for file in val_patches]\n",
    "\n",
    "# CSV Header file\n",
    "header_file = f'{data_dir}/header.csv'\n",
    "\n",
    "# Number of samples to save/put aside for meta learner\n",
    "meta_train_samples = 1e4\n",
    "meta_val_samples = 2.5e3\n",
    "\n",
    "# Splits per compartment\n",
    "K_SPLITS = 1\n",
    "\n",
    "# Create save directory\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Load data file and indexes\n",
    "base_trainset = [np.genfromtxt(file, delimiter=',', skip_header=True, dtype='unicode') for file in training_files]\n",
    "valset = [np.genfromtxt(file, delimiter=',', skip_header=True, dtype='unicode') for file in validate_files]\n",
    "\n",
    "header = ','.join(list(np.genfromtxt(header_file, delimiter=',', dtype='unicode')))\n",
    "\n",
    "# Sample meta data\n",
    "train_indices = [np.random.choice(comp.shape[0], int(meta_train_samples/len(base_trainset)), replace=False) for comp in base_trainset]\n",
    "val_indices = [np.random.choice(comp.shape[0], int(meta_val_samples/len(valset)), replace=False) for comp in valset]\n",
    "\n",
    "# Extract indices to create disjoint meta / base sets\n",
    "metatrain = [[data[indices] for j, indices in enumerate(train_indices) if j == i] for i, data in enumerate(base_trainset)]\n",
    "metaval = [[data[indices] for j, indices in enumerate(val_indices) if j == i] for i, data in enumerate(valset)]\n",
    "\n",
    "basetrain = [[np.delete(data,indices, axis=0) for j, indices in enumerate(train_indices) if j == i] for i, data in enumerate(base_trainset)]\n",
    "baseval = [[np.delete(data,indices, axis=0) for j, indices in enumerate(val_indices) if j == i] for i, data in enumerate(valset)]\n",
    "\n",
    "for (compartment, train, val) in zip([\"aorta\", \"cerebro\", \"cardiac\"], basetrain, baseval):\n",
    "    train = train[0]\n",
    "    val = val[0]\n",
    "    np.random.shuffle(train)\n",
    "    np.random.shuffle(val)\n",
    "    \n",
    "    base_train_splits = np.array_split(train, K_SPLITS)\n",
    "    val_splits = np.array_split(val, K_SPLITS)\n",
    "    \n",
    "    # Save training splits\n",
    "    for k, t_splits in enumerate(base_train_splits):\n",
    "        name = f'{save_dir}/{compartment}{k}_base_patches.csv'\n",
    "        np.savetxt(name, t_splits, fmt='%s',delimiter=',', comments='', header=header)\n",
    "    \n",
    "    # Save validation splits \n",
    "    for k, v_splits in enumerate(val_splits):\n",
    "        name = f'{save_dir}/{compartment}{k}_val_patches.csv'\n",
    "        np.savetxt(name, v_splits, fmt='%s',delimiter=',', comments='', header=header)\n",
    "\n",
    "\n",
    "# Combine meta data across compartments\n",
    "metatrain = np.concatenate([comp[0] for comp in metatrain], axis=0)\n",
    "metaval = np.concatenate([comp[0] for comp in metaval], axis=0)\n",
    "\n",
    "# Shuffle for good measure\n",
    "np.random.shuffle(metatrain) \n",
    "np.random.shuffle(metaval)  \n",
    "\n",
    "# Save\n",
    "np.savetxt(f'{save_dir}/meta_train_patches.csv', metatrain, fmt='%s',delimiter=',', comments='', header=header)\n",
    "np.savetxt(f'{save_dir}/meta_val_patches.csv', metaval, fmt='%s',delimiter=',', comments='', header=header)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells below have been extracted into seperate script files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "data_dir = \"../data\"\n",
    "validate_file = f\"{data_dir}/val_patches.csv\"\n",
    "benchmark_file = f\"{data_dir}/test_patches.csv\"\n",
    "\n",
    "for i in range(NUM_BASE_MODELS):\n",
    "    for k in range(K_FOLDS):\n",
    "        training_file = f\"{data_dir}/fold{k}_base_patches.csv\"\n",
    "        network_name = f\"4DFlowNet-stacking{i}-fold{k}\"\n",
    "        subprocess.call(f\"bash ../../train_base.sbatch {data_dir} {training_file} {validate_file} {benchmark_file} {network_name}\", shell=True)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating training / validation data for meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import h5py\n",
    "import os\n",
    "from Network.PatchHandler3D import PatchHandler3D\n",
    "from utils import prediction_utils\n",
    "\n",
    "\n",
    "def save(predictions, hr, venc, mask, compartment, output_filepath, models):\n",
    "    \"\"\"Save generated predictions (meta-learner input) along with the rest of the training data as a single HDF5 file\"\"\"\n",
    "    \n",
    "    # Base model predictions - U, V, W\n",
    "    for i in range(0, predictions.shape[-1], 3):\n",
    "        prediction_utils.save_to_h5(output_filepath, f'u_m{i}', predictions[:,:,:,:i], compression='gzip')\n",
    "        prediction_utils.save_to_h5(output_filepath, f'v_m{i}', predictions[:,:,:,:i+1], compression='gzip')\n",
    "        prediction_utils.save_to_h5(output_filepath, f'w_m{i}', predictions[:,:,:,:i+2], compression='gzip')\n",
    "        \n",
    "    # Model names\n",
    "    models = np.asarray(models, dtype=h5py.special_dtype(vlen=str))\n",
    "    prediction_utils.save_to_h5(output_filepath, 'base_models', models, compression='gzip')\n",
    "\n",
    "    # HR - U, V, W\n",
    "    prediction_utils.save_to_h5(output_filepath, 'u_hr', hr[0], compression='gzip')\n",
    "    prediction_utils.save_to_h5(output_filepath, 'v_hr', hr[1], compression='gzip')\n",
    "    prediction_utils.save_to_h5(output_filepath, 'w_hr', hr[2], compression='gzip')\n",
    "    \n",
    "    # VENC, Mask, Compartment\n",
    "    prediction_utils.save_to_h5(output_filepath, 'venc', venc, compression='gzip')\n",
    "    prediction_utils.save_to_h5(output_filepath, 'mask', mask, compression='gzip')\n",
    "    prediction_utils.save_to_h5(output_filepath, 'compartment', compartment, compression='gzip')\n",
    "    return\n",
    "\n",
    "def load_indexes(index_file):\n",
    "    \"\"\"\n",
    "        Load patch index file (csv). This is the file that is used to load the patches based on x,y,z index\n",
    "    \"\"\"\n",
    "    indexes = np.genfromtxt(index_file, delimiter=',', skip_header=True, dtype='unicode') # 'unicode' or None\n",
    "    return indexes\n",
    "\n",
    "def load_model(model_path, model):\n",
    "    name = model[:model.rindex(\"_\")]\n",
    "    model = tf.keras.models.load_model(f\"{model_path}/{name}-best.h5\")\n",
    "    return  model\n",
    "\n",
    "data_dir = '../data'\n",
    "stacking_dir = f\"{data_dir}/stacking\"\n",
    "\n",
    "data_files = [f\"{stacking_dir}/fold{k}_meta_patches.csv\" for k in range(K_FOLDS)]\n",
    "\n",
    "model_dir = \"../models\"\n",
    "output_dir = f\"../results/stacking_compartment\"\n",
    "\n",
    "# Folds and their corresponding models - Adjust for training/validation\n",
    "fold_model_set = {\n",
    "    \"fold0_meta_patches.csv\": [\"4DFlowNet-stacking1_20230303-1415\", \n",
    "                                \"4DFlowNet-stacking2_20230303-1433\", \"4DFlowNet-stacking3_20230303-1435\"],\n",
    "    \"fold1_meta_patches.csv\": [\"4DFlowNet-stacking1_20230303-1415\", \n",
    "                                \"4DFlowNet-stacking2_20230303-1433\", \"4DFlowNet-stacking3_20230303-1435\"],\n",
    "    \"fold2_meta_patches.csv\": [\"4DFlowNet-stacking1_20230303-1415\", \n",
    "                                \"4DFlowNet-stacking2_20230303-1433\", \"4DFlowNet-stacking3_20230303-1435\"],\n",
    "}\n",
    "\n",
    "# Params\n",
    "patch_size = 12\n",
    "res_increase = 2\n",
    "batch_size = 64\n",
    "mask_threshold = 0.6\n",
    "round_small_values = True\n",
    "sr_patch_size = patch_size*res_increase\n",
    "\n",
    "# Network\n",
    "low_resblock=8\n",
    "hi_resblock=4\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# Iterate over the data and generate predictions\n",
    "for fold_idx, (fold_name, model_names) in enumerate(fold_model_set.items()):\n",
    "    fold_data = load_indexes(f\"{stacking_dir}/{fold_name}\")\n",
    "    fold_models = [load_model(f\"{model_dir}/{m}\", m) for m in model_names]\n",
    "\n",
    "    ph = PatchHandler3D(data_dir, patch_size, res_increase, batch_size, mask_threshold)\n",
    "    metaset = ph.initialize_dataset(fold_data, shuffle=False, n_parallel=None, drop_remainder=True)\n",
    "\n",
    "    num_batches = tf.data.experimental.cardinality(metaset).numpy()\n",
    "    nr_samples = num_batches * batch_size\n",
    "    \n",
    "    \n",
    "    print(f\"Fold {fold_idx}, number of batches: {num_batches}\")\n",
    "    predictions = np.zeros((nr_samples, sr_patch_size, sr_patch_size, sr_patch_size, 3*NUM_BASE_MODELS), dtype='float32')\n",
    "    hr = np.zeros((3, nr_samples, sr_patch_size, sr_patch_size, sr_patch_size), dtype='float32')\n",
    "    venc = np.zeros((nr_samples), dtype='float32')\n",
    "    mask = np.zeros((nr_samples, sr_patch_size, sr_patch_size, sr_patch_size), dtype='float32')\n",
    "    compartment = np.zeros((nr_samples), dtype=h5py.special_dtype(vlen=str))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Iterate over the data and generate predictions\n",
    "    for batch_idx, (data_batch) in enumerate(metaset): \n",
    "        fill_range = range(batch_idx*batch_size, (batch_idx*batch_size + batch_size))\n",
    "            \n",
    "        lr_input = data_batch[:6]\n",
    "        hr[:, fill_range] = np.squeeze(np.asarray(data_batch[6:9]))\n",
    "        venc[fill_range], mask[fill_range], compartment[fill_range] = (data.numpy() for data in data_batch[9:12])\n",
    "        \n",
    "        batch_predictions = np.zeros((batch_size, sr_patch_size, sr_patch_size, sr_patch_size, 0))\n",
    "        \n",
    "        for model_idx, model in enumerate(fold_models):\n",
    "            # Predict using 3D velocities and 3D magnitudes\n",
    "            sr_images = model(lr_input, training=False)\n",
    "            \n",
    "            # Denormalize\n",
    "            sr_images = sr_images * venc[fill_range].reshape(-1,1,1,1,1)\n",
    "            \n",
    "            # Concatenate along the channel axis\n",
    "            batch_predictions = np.concatenate((batch_predictions,sr_images), axis=-1) # (Batch size, 24, 24, 24, 3 * NUM_BASE_MODELS)\n",
    "        \n",
    "        predictions[fill_range] = batch_predictions # (Samples, 24, 24, 24, 3 * NUM_BASE_MODELS)\n",
    "        \n",
    "        # Logging\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"\\rProcessed {batch_idx}/{num_batches} Elapsed: {(time.time() - start_time):.2f} secs.\", end='\\r')\n",
    "    time_taken = time.time() - start_time\n",
    "\n",
    "    print(f\"Processed fold {fold_idx}, Elapsed: {(time.time() - start_time):.2f} secs.\")\n",
    "    \n",
    "    # Save the meta-learner training data patch-wise\n",
    "    print(\"Saving fold...\")\n",
    "    save(predictions, hr, venc, mask, compartment, f'{output_dir}/meta_training.h5', model_names)\n",
    "    print(f\"Saved fold {fold_idx}, Elapsed: {(time.time() - start_time):.2f} secs.\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train meta-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use meta_trainer.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save subset of h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_h5(output_filepath, col_name, dataset, chunks=True):\n",
    "    #dataset = np.expand_dims(dataset, axis=0)\n",
    "\n",
    "    # convert float64 to float32 to save space\n",
    "    if dataset.dtype == 'float64':\n",
    "        dataset = np.array(dataset, dtype='float32')\n",
    "    \n",
    "    with h5py.File(output_filepath, 'a') as hf:    \n",
    "        if col_name not in hf:\n",
    "            datashape = (None, )\n",
    "            if (dataset.ndim > 1):\n",
    "                datashape = (None, ) + dataset.shape[1:]\n",
    "            hf.create_dataset(col_name, data=dataset, maxshape=datashape, chunks=chunks, compression=\"lzf\")\n",
    "        else:\n",
    "            hf[col_name].resize((hf[col_name].shape[0]) + dataset.shape[0], axis = 0)\n",
    "            hf[col_name][-dataset.shape[0]:] = dataset\n",
    "\n",
    "file = \"../data/stacking_small/meta_training.h5\"           \n",
    "frame = 10\n",
    "\n",
    "rows = ['u_hr', 'v_hr', 'w_hr', 'u_m0', 'u_m1', 'u_m2', 'v_m0', 'v_m1', 'v_m2', 'w_m0', 'w_m1', 'w_m2', 'venc', 'mask', 'compartment']\n",
    "model_r = ['u_hr', 'v_hr', 'w_hr', 'venc', 'mask', 'compartment']\n",
    "data = {}\n",
    "with h5py.File(file, 'r') as hf:\n",
    "    for r in rows:\n",
    "        data[r] = np.asarray(hf[r])\n",
    "    \n",
    "for colname, d in data.items():\n",
    "    if colname in ['u_hr', 'v_hr', 'w_hr', 'u_m0', 'u_m1', 'u_m2', 'v_m0', 'v_m1', 'v_m2', 'w_m0', 'w_m1', 'w_m2']:\n",
    "        save_to_h5(\"../data/stacking_small/meta_training_10chunks.h5\", colname, d, chunks=(10, 24, 24, 24))\n",
    "    else:\n",
    "        save_to_h5(\"../data/stacking_small/meta_training_10chunks.h5\", colname, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"../data/stacking_splits/meta_training.h5\", 'r') as hf:\n",
    "    for k in hf.keys():\n",
    "        print(hf[k]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(data, venc):\n",
    "    return data / venc\n",
    "\n",
    "def load_patches_from_h5(idx):\n",
    "    with h5py.File(training_file, 'r') as hf:\n",
    "        with h5py.File(self.file, 'r') as hf:\n",
    "        mask, venc, compartment = [tf.convert_to_tensor(hf[ds][idx]) for ds in ['mask', 'venc', 'compartment']]\n",
    "        u_hr, v_hr, w_hr = [self._normalize(tf.convert_to_tensor(hf[ds][idx]), venc) for ds in ['u_hr', 'v_hr', 'w_hr']]\n",
    "        uhv_m = [] \n",
    "        for m_idx in range(3): # change to number of base models\n",
    "            uhv_m.append(self._normalize(hf[f'u_m{m_idx}'][idx], venc))\n",
    "            uhv_m.append(self._normalize(hf[f'v_m{m_idx}'][idx], venc))\n",
    "            uhv_m.append(self._normalize(hf[f'w_m{m_idx}'][idx], venc))\n",
    "        return tf.convert_to_tensor(uhv_m), u_hr[tf.newaxis], v_hr[tf.newaxis], w_hr[tf.newaxis], venc, mask, compartment\n",
    "                \n",
    "def load_data_using_patch_index(idx):\n",
    "    return tf.py_function(func=load_patches_from_h5, \n",
    "        # U-LR, HR, MAG, V-LR, HR, MAG, W-LR, HR, MAG, venc, MASK\n",
    "        inp=[idx], \n",
    "            Tout=[tf.float32, tf.float32, tf.float32,\n",
    "                tf.float32, tf.float32, tf.float32,\n",
    "                tf.string])\n",
    "\n",
    "idx = tf.range(10000)\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((idx)) \n",
    "\n",
    "ds = ds.shuffle(buffer_size=46656) \n",
    "\n",
    "ds = ds.map(load_data_using_patch_index)\n",
    "ds = ds.batch(batch_size=batch_size, drop_remainder=False)\n",
    "\n",
    "# prefetch, n=number of items\n",
    "ds = ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(ds):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator:\n",
    "    def __init__(self, file, base_models):\n",
    "        self.file = file\n",
    "        self.base_models = base_models\n",
    "    \n",
    "    def _normalize(self, data, venc):\n",
    "        return data / venc\n",
    "\n",
    "    def __call__(self):\n",
    "        with h5py.File(self.file, 'r') as hf:\n",
    "            nr_patches = hf['compartment'].shape[0]\n",
    "            mask = hf['mask']\n",
    "            for p_idx in range(nr_patches):\n",
    "                mask, venc, compartment = [tf.convert_to_tensor(hf[ds][p_idx]) for ds in ['mask', 'venc', 'compartment']]\n",
    "                u_hr, v_hr, w_hr = [self._normalize(tf.convert_to_tensor(hf[ds][p_idx]), venc) for ds in ['u_hr', 'v_hr', 'w_hr']]\n",
    "                uhv_m = []\n",
    "                for m_idx in range(self.base_models):\n",
    "                    uhv_m.append(self._normalize(hf[f'u_m{m_idx}'][p_idx], venc))\n",
    "                    uhv_m.append(self._normalize(hf[f'v_m{m_idx}'][p_idx], venc))\n",
    "                    uhv_m.append(self._normalize(hf[f'w_m{m_idx}'][p_idx], venc))\n",
    "                yield tf.convert_to_tensor(uhv_m), u_hr, v_hr, w_hr, mask, venc, compartment\n",
    "\n",
    "patch = (patch_size, patch_size, patch_size)\n",
    "ds = tf.data.Dataset.from_generator(\n",
    "       generator(training_file, NUM_BASE_MODELS), \n",
    "       output_signature=(\n",
    "           tf.TensorSpec(shape=((NUM_BASE_MODELS*3,) + patch), dtype=tf.float32, name='uhv_m'),\n",
    "           tf.TensorSpec(shape=patch, dtype=tf.float32, name='u_hr'),\n",
    "           tf.TensorSpec(shape=patch, dtype=tf.float32, name='v_hr'),\n",
    "           tf.TensorSpec(shape=patch, dtype=tf.float32, name='w_hr'),\n",
    "           tf.TensorSpec(shape=patch, dtype=tf.float32, name='mask'),\n",
    "           tf.TensorSpec(shape=(), dtype=tf.float32, name='venc'),\n",
    "           tf.TensorSpec(shape=(), dtype=tf.string, name='compartment')\n",
    "       ))\n",
    "\n",
    "#ds = ds.shuffle(10) \n",
    "\n",
    "ds = ds.batch(batch_size=batch_size)\n",
    "\n",
    "# prefetch, n=number of items\n",
    "ds = ds.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network.loss_utils import calculate_relative_error\n",
    "\n",
    "hf = h5py.File(training_file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are using 4GB of chunk_cache_mem here (\"rdcc_nbytes\")\n",
    "shape = (6912, 24, 24, 24)\n",
    "shape2 = (6912,)\n",
    "data = np.random.rand(6912, 24, 24, 24)\n",
    "chunk_shape=(10, 24, 24, 24)\n",
    "\n",
    "file = \"test_c10_lzf_x.h5\"\n",
    "#f = h5py.File(file, 'w',rdcc_nbytes =1024**2*4000,rdcc_nslots=1e7)\n",
    "with h5py.File(file, 'a') as f:\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'u_m0', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'u_m1', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'u_m2', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'v_m0', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'v_m1', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'v_m2', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'w_m0', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'w_m1', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'w_m2', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'w_m3', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'u_hr', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'v_hr', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'w_hr', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912, 24, 24, 24)\n",
    "    f.create_dataset(f'mask', shape ,dtype=np.float32,data=data,chunks=chunk_shape,compression=\"lzf\")\n",
    "    data = np.random.rand(6912)\n",
    "    f.create_dataset(f'venc', shape2 ,dtype=np.float32,data=data,compression=\"gzip\")\n",
    "    f.create_dataset(f'compartment', shape2 ,dtype=np.float32,data=data,compression=\"gzip\")\n",
    "hftest = h5py.File(file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hftest = h5py.File(\"test_c10_lzf.h5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rnd = np.random.choice(6912, 500, replace=False)\n",
    "start = time.time()\n",
    "for i in rnd:\n",
    "    x = hftest['u_hr'][i]\n",
    "    x = hftest['w_hr'][i]\n",
    "    x = hftest['v_hr'][i]\n",
    "    x = hftest['u_m0'][i]\n",
    "    x = hftest['u_m1'][i]\n",
    "    x = hftest['u_m2'][i]\n",
    "    x = hftest['v_m0'][i]\n",
    "    x = hftest['v_m1'][i]\n",
    "    x = hftest['v_m2'][i]\n",
    "    x = hftest['w_m0'][i]\n",
    "    x = hftest['w_m1'][i]\n",
    "    x = hftest['w_m2'][i]\n",
    "print(f\"Elapsed: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for i in rnd:\n",
    "    x = hf['u_hr'][i]\n",
    "    x = hf['w_hr'][i]\n",
    "    x = hf['v_hr'][i]\n",
    "    x = hf['u_m0'][i]\n",
    "    x = hf['u_m1'][i]\n",
    "    x = hf['u_m2'][i]\n",
    "    x = hf['v_m0'][i]\n",
    "    x = hf['v_m1'][i]\n",
    "    x = hf['v_m2'][i]\n",
    "    x = hf['w_m0'][i]\n",
    "    x = hf['w_m1'][i]\n",
    "    x = hf['w_m2'][i]\n",
    "print(f\"Elapsed: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 0\n",
    "mask, venc, compartment = [tf.convert_to_tensor(hf[ds][sample]) for ds in ['mask', 'venc', 'compartment']]\n",
    "u_hr, v_hr, w_hr = [_normalize(tf.convert_to_tensor(hf[ds][sample]), venc) for ds in ['u_hr', 'v_hr', 'w_hr']]    \n",
    "uvw_m = []\n",
    "for m_idx in range(3):\n",
    "    uvw_m.append(_normalize(hf[f'u_m{m_idx}'][sample], venc))\n",
    "    uvw_m.append(_normalize(hf[f'v_m{m_idx}'][sample], venc))\n",
    "    uvw_m.append(_normalize(hf[f'w_m{m_idx}'][sample], venc))\n",
    "uvw_m = tf.convert_to_tensor(uvw_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uvw_m.shape, u_hr.shape, v_hr.shape, w_hr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_hr = tf.expand_dims(u_hr, 0)\n",
    "v_hr = tf.expand_dims(v_hr, 0)\n",
    "w_hr = tf.expand_dims(w_hr, 0)\n",
    "mask = tf.expand_dims(mask, 0)\n",
    "\n",
    "for i in range(0, uvw_m.shape[0], 3):\n",
    "    u = tf.expand_dims(uvw_m[i], 0)\n",
    "    v = tf.expand_dims(uvw_m[i+1], 0)\n",
    "    w = tf.expand_dims(uvw_m[i+2], 0)\n",
    "    \n",
    "    batch_accuracy = calculate_relative_error(u, v, w, u_hr, v_hr, w_hr, mask)\n",
    "    print(f\"Model {i//3} accuracy: {tf.reduce_mean(batch_accuracy)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1\n",
    "b = False\n",
    "x = 1 / (y) if b else 2\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"../data/stacking_splits/meta_training.h5\", 'r') as hf:\n",
    "    for k in hf.keys():\n",
    "        print(hf[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2def03effaa4664ca4ac745ff7bd8b4c254a5de39fb17f6881088c842d88f04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
